defaults:
  - _base_
  - _self_

experiment_name: fp16+gptq+exllamav2

backend:
  no_weights: true
  quantization_scheme: gptq
  quantization_config:
    exllama_config:
      version: 2

hydra:
  sweeper:
    params:
      model: TheBloke/LLaMa-7B-GPTQ,TheBloke/LLaMa-13B-GPTQ,TheBloke/LLaMa-65B-GPTQ
